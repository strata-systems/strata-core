# M1 Foundation Quality Audit and Testing Plan

**Created**: 2026-01-11
**Status**: DRAFT - Awaiting execution
**Priority**: CRITICAL - Must complete before Epic 5

---

## Executive Summary

**Problem Identified**:
- Epics 1 and 2 may have tests that were adjusted to pass instead of fixing underlying bugs
- No documentation of test failures or workarounds
- Unknown actual test coverage quality
- Risk of silent bugs in M1 foundation

**Impact**:
- **HIGH** - M1 is the foundation for all future work
- Bugs in core types, storage, or recovery will cascade to all features
- Technical debt will compound exponentially

**Proposed Solution**:
Multi-phase quality audit with mutation testing, coverage analysis, and systematic bug hunting before proceeding with Epic 5.

---

## Phase 1: Test Correctness Review (Manual Inspection)

### Objective
**CRITICAL**: Validate that every test in Epics 1-3 is testing the RIGHT behavior, not just passing behavior.

**The Problem**: Agent execution can silently rewrite tests during implementation, leaving no trace in git history or comments. The final commit looks "clean" but the test may be wrong.

### Methodology

#### 1.1 Test Inventory
```bash
# List ALL tests in Epics 1-3
echo "=== Epic 1: Core Types ==="
fd -e rs -x rg "^\s*#\[test\]" -A 1 {} crates/core/

echo "=== Epic 2: Storage Layer ==="
fd -e rs -x rg "^\s*#\[test\]" -A 1 {} crates/storage/

echo "=== Epic 3: WAL Implementation ==="
fd -e rs -x rg "^\s*#\[test\]" -A 1 {} crates/durability/
```

**Expected counts** (from PROJECT_STATUS.md):
- Epic 1: ~75 tests
- Epic 2: ~87 tests
- Epic 3: ~54 tests
- **Total: ~216 tests to review**

#### 1.2 Test Review Checklist (Per Test)

For **EVERY test** in Epics 1-3, ask:

**1. Does this test have a clear purpose?**
- [ ] Test name describes what's being tested
- [ ] Test name describes expected behavior
- [ ] Test follows naming: `test_{component}_{scenario}_{expected}`

**2. Is the test asserting the RIGHT thing?**
- [ ] Assertions match the specification (not just "what the code does")
- [ ] Assertions are strict enough (e.g., `assert_eq!` not just `assert!`)
- [ ] Error cases have specific error type checks (not just `.is_err()`)
- [ ] Edge cases are tested (not just happy path)

**3. Is the test data realistic?**
- [ ] Test data exercises the actual use case
- [ ] Test data not crafted to avoid a specific bug
- [ ] Test data includes boundary conditions

**4. Are there suspicious patterns?**
- [ ] Test uses `.unwrap()` excessively (may be hiding errors)
- [ ] Test has commented-out assertions
- [ ] Test has TODO/FIXME comments
- [ ] Test logic is overly complex (may be working around a bug)
- [ ] Test has multiple unrelated assertions (testing too much)

**5. Does the test actually fail if we break the code?**
- [ ] Mark for mutation testing (Phase 4)

#### 1.3 Systematic Review Process

**Step 1: Create Test Review Spreadsheet**
```
Test Name | File | Purpose | Assertions | Concerns | Status
----------|------|---------|------------|----------|--------
test_runid_new | core/types.rs:45 | RunId creation | assert_ne | None | ‚úÖ
test_key_ordering | core/types.rs:67 | BTree ordering | assert!(a<b) | Weak assertion? | ‚ö†Ô∏è
...
```

**Step 2: Review Core Types Tests** (Epic 1)
Files to review:
- `crates/core/src/types.rs` (mod tests)
- `crates/core/src/value.rs` (mod tests)
- `crates/core/src/error.rs` (mod tests)
- `crates/core/tests/*.rs` (if any)

**Common issues to look for**:
- RunId uniqueness: Does test verify UUIDs are actually unique? (generate 1000, check no duplicates)
- Key ordering: Does test verify ordering matches spec? (not just "sorts somehow")
- Value serialization: Does test roundtrip ALL variants? (not just a few)
- Error conversions: Does test verify error messages? (not just error type)

**Step 3: Review Storage Tests** (Epic 2)
Files to review:
- `crates/storage/src/unified.rs` (mod tests)
- `crates/storage/src/index.rs` (mod tests)
- `crates/storage/src/ttl.rs` (mod tests)
- `crates/storage/src/snapshot.rs` (mod tests)
- `crates/storage/tests/*.rs` (if any)

**Critical areas**:
- **Concurrent access**: Do tests actually run multi-threaded? (not just claim to)
- **Secondary indices**: Do tests verify indices ALWAYS match main store? (check after every operation)
- **TTL expiration**: Do tests verify expired entries are REALLY gone? (not just "marked expired")
- **Version counter**: Do tests verify versions are monotonic and gap-free?

**Step 4: Review WAL Tests** (Epic 3)
Files to review:
- `crates/durability/src/wal.rs` (mod tests)
- `crates/durability/src/encoding.rs` (mod tests)
- `crates/durability/tests/corruption_test.rs`
- `crates/durability/tests/corruption_simulation_test.rs`

**Known good** (from Epic 3 review):
- Issue #51 discovered and properly fixed
- 16 corruption scenarios tested
- TDD integrity verified

**Still check**:
- Corruption tests: Do they test corruption at EVERY possible offset? (not just a few)
- Durability modes: Do tests verify fsync actually happens? (not just "no error")
- Recovery: Do tests verify data EXACTLY matches pre-crash? (not just "something recovered")

#### 1.4 Deep Dive: Suspicious Test Examples

**Example 1: Weak Assertion**
```rust
// ‚ùå BAD - Too weak, doesn't verify actual behavior
#[test]
fn test_key_ordering() {
    let k1 = Key::new(...);
    let k2 = Key::new(...);
    assert!(k1 != k2); // Just checks they're different, not ordering
}

// ‚úÖ GOOD - Verifies specific ordering
#[test]
fn test_key_ordering() {
    let k1 = Key::new_kv(ns.clone(), b"aaa");
    let k2 = Key::new_kv(ns.clone(), b"bbb");
    assert!(k1 < k2, "Keys should be ordered by user_key");
    assert_eq!(k1.cmp(&k2), Ordering::Less);
}
```

**Example 2: Hidden Error**
```rust
// ‚ùå BAD - unwrap hides if get() returns wrong error
#[test]
fn test_get_nonexistent() {
    let store = UnifiedStore::new();
    let result = store.get(&key).unwrap(); // Should this unwrap?
    assert!(result.is_none());
}

// ‚úÖ GOOD - Explicit error handling
#[test]
fn test_get_nonexistent() {
    let store = UnifiedStore::new();
    match store.get(&key) {
        Ok(None) => {}, // Expected
        Ok(Some(_)) => panic!("Found nonexistent key"),
        Err(e) => panic!("Unexpected error: {:?}", e),
    }
}
```

**Example 3: Test Data Crafted to Avoid Bug**
```rust
// ‚ùå SUSPICIOUS - Why specifically 100 bytes? Avoiding a bug at 0 or max?
#[test]
fn test_large_value() {
    let value = vec![0u8; 100]; // Why 100?
    store.put(key, value).unwrap();
}

// ‚úÖ BETTER - Test actual boundaries
#[test]
fn test_value_sizes() {
    // Empty
    store.put(key.clone(), vec![]).unwrap();
    // Small
    store.put(key.clone(), vec![1]).unwrap();
    // Large (1MB)
    store.put(key.clone(), vec![0u8; 1024*1024]).unwrap();
}
```

#### 1.5 Cross-Reference with Specification

For each test, cross-reference with:
- **M1_ARCHITECTURE.md** - Does test match spec?
- **TDD_METHODOLOGY.md** - Does test follow TDD principles?
- **GitHub issue** - Does test cover acceptance criteria?

**Red flag**: Test passes but doesn't cover any acceptance criteria from the issue.

#### 1.6 Create Test Correctness Report

**Template**: `docs/milestones/TEST_CORRECTNESS_REPORT.md`

```markdown
# Test Correctness Report - Epic 1

## Summary
- Total tests reviewed: X
- Tests with concerns: Y
- Tests flagged for rewrite: Z

## Core Types (crates/core)

### ‚úÖ Correct Tests
- `test_runid_new` - Verifies UUID generation
- `test_namespace_equality` - Proper equality check
...

### ‚ö†Ô∏è Concerns
- `test_key_ordering` (types.rs:67)
  - **Issue**: Weak assertion, only checks inequality
  - **Should test**: Specific ordering by namespace ‚Üí type_tag ‚Üí user_key
  - **Action**: Strengthen test with ordering verification

### ‚ùå Incorrect Tests (Rewrite Required)
- `test_value_serialization` (value.rs:123)
  - **Issue**: Only tests Bytes variant, ignores Event/StateMachine/Trace
  - **Should test**: ALL 5 Value enum variants
  - **Action**: Rewrite to test all variants

## Storage Layer (crates/storage)
...
```

### Deliverable
**Test Correctness Report** with:
- ‚úÖ CORRECT - Tests verify right behavior
- ‚ö†Ô∏è  CONCERNS - Tests need strengthening
- ‚ùå INCORRECT - Tests must be rewritten

**Action Items**:
- List of tests to strengthen (with specific changes)
- List of tests to rewrite (with reason)
- List of MISSING tests (gaps in coverage)

---

## Phase 2: Full Test Suite Validation

### Objective
Verify all tests still pass in all configurations and identify any flaky or environment-dependent tests.

### Methodology

#### 2.1 Multi-Configuration Testing
```bash
# Debug mode (default)
cargo test --all 2>&1 | tee test-results-debug.log

# Release mode (optimizations enabled)
cargo test --all --release 2>&1 | tee test-results-release.log

# Single-threaded (catches race conditions)
cargo test --all -- --test-threads=1 2>&1 | tee test-results-single-thread.log

# With all features (if any)
cargo test --all --all-features 2>&1 | tee test-results-all-features.log

# Run 10 times to catch flaky tests
for i in {1..10}; do
  echo "=== Run $i ==="
  cargo test --all 2>&1 | tee -a test-results-flaky-check.log
done
```

#### 2.2 Test Failure Analysis
For any failures:
- [ ] Is the test correct (testing the right behavior)?
- [ ] Is the implementation buggy?
- [ ] Is the test flaky (non-deterministic)?
- [ ] Is it a real bug or environmental issue?

#### 2.3 Test Organization Review
```bash
# Count tests per crate
cargo test --all -- --list | grep "test " | wc -l

# List all ignored tests (should be rare)
cargo test --all -- --ignored --list

# Check for tests without assertions (ineffective tests)
rg "^[[:space:]]*fn test_" -A 20 crates/ | grep -L "assert"
```

### Deliverable
**Test Suite Validation Report**:
- Total test count per crate
- Pass/fail rates across configurations
- List of flaky tests (if any)
- Environmental dependencies identified

---

## Phase 3: Code Coverage Analysis (Deep Dive)

### Objective
Measure actual line coverage and identify untested code paths, especially error handling.

### Methodology

#### 3.1 Install Coverage Tools
```bash
# Install tarpaulin (coverage tool)
cargo install cargo-tarpaulin

# Install grcov (alternative coverage tool)
cargo install grcov
rustup component add llvm-tools-preview
```

#### 3.2 Generate Coverage Reports
```bash
# Tarpaulin HTML report
cargo tarpaulin --all --out Html --output-dir coverage/tarpaulin \
  --exclude-files 'tests/*' --timeout 300

# Per-crate coverage
for crate in core storage concurrency durability engine; do
  echo "=== Coverage for $crate ==="
  cargo tarpaulin -p in-mem-$crate --out Html \
    --output-dir coverage/$crate --timeout 300
done

# Generate line-by-line coverage report
cargo tarpaulin --all --out Json --output-dir coverage/json
```

#### 3.3 Coverage Analysis Checklist
- [ ] **Core types** (target: 100%)
  - RunId, Namespace, Key, TypeTag, Value enum
  - All helper methods covered
  - Error types and conversions

- [ ] **Storage layer** (target: ‚â•95%)
  - UnifiedStore: put, get, delete, scan operations
  - Secondary indices: run_index, type_index
  - TTL expiration logic
  - ClonedSnapshotView

- [ ] **Error paths** (CRITICAL - often untested!)
  - I/O errors (file operations)
  - Serialization errors
  - Validation errors
  - Out-of-bounds errors

#### 3.4 Identify Coverage Gaps
```bash
# Find uncovered lines in core
cargo tarpaulin -p in-mem-core --out Json | \
  jq '.files[] | select(.coverage < 100) | {file: .path, coverage: .coverage}'

# Find uncovered lines in storage
cargo tarpaulin -p in-mem-storage --out Json | \
  jq '.files[] | select(.coverage < 95) | {file: .path, coverage: .coverage}'
```

### Deliverable
**Coverage Report**:
- Per-crate coverage percentages
- Line-by-line coverage visualization (HTML)
- List of uncovered critical paths
- Specific lines/functions needing tests

---

## Phase 4: Mutation Testing (Advanced Bug Detection)

### Objective
Verify that tests actually catch bugs by introducing mutations and checking if tests fail.

**What is Mutation Testing?**
- Automatically modify code (e.g., change `>` to `>=`, flip boolean conditions)
- Run tests against mutated code
- If tests still pass, the mutation "survived" ‚Üí **weak tests**
- High "kill rate" = strong tests

### Methodology

#### 4.1 Install Mutation Testing Tool
```bash
# Install cargo-mutants
cargo install cargo-mutants

# Or use mutagen (alternative)
cargo install cargo-mutagen
```

#### 4.2 Run Mutation Tests on Core Components
```bash
# Mutate core types
cargo mutants --package in-mem-core \
  --output mutants-core.json \
  --timeout 120

# Mutate storage layer
cargo mutants --package in-mem-storage \
  --output mutants-storage.json \
  --timeout 120

# Analyze mutation results
cargo mutants --list-files mutants-core.json
```

#### 4.3 Mutation Analysis
For each survived mutant:
- [ ] Why didn't tests catch this mutation?
- [ ] Is the mutated code actually reachable?
- [ ] Is there a missing test case?
- [ ] Is the code redundant/dead?

**Example Mutations**:
```rust
// Original
if version > 0 { ... }

// Mutation 1: Boundary condition
if version >= 0 { ... }  // Should fail if tests check version=0

// Mutation 2: Logic flip
if version <= 0 { ... }  // Should fail if tests check version>0

// Mutation 3: Constant change
if version > 1 { ... }   // Should fail if tests check version=1
```

### Deliverable
**Mutation Testing Report**:
- Mutation score per crate (% of mutants killed)
- List of survived mutants (weak tests)
- Recommended test additions

**Target Mutation Score**: ‚â•80% for core and storage

---

## Phase 5: Property-Based Testing (Fuzzing)

### Objective
Use property-based testing to find edge cases and unexpected behavior.

### Methodology

#### 5.1 Identify Properties to Test

**Core Types Properties**:
- RunId serialization roundtrip: `deserialize(serialize(x)) == x`
- Key ordering: `a < b ‚áí a.cmp(b) == Less`
- Namespace equality: `a == b ‚áí a.hash() == b.hash()`

**Storage Properties**:
- Put-Get consistency: `storage.put(k, v); storage.get(k) == Some(v)`
- Delete idempotence: `storage.delete(k); storage.delete(k) == None`
- Scan ordering: Results sorted by key
- TTL expiration: Value absent after TTL expires

#### 5.2 Write Property Tests
```rust
// Example property test for Key ordering
use proptest::prelude::*;

proptest! {
    #[test]
    fn key_ordering_transitive(
        k1 in arbitrary_key(),
        k2 in arbitrary_key(),
        k3 in arbitrary_key()
    ) {
        if k1 < k2 && k2 < k3 {
            assert!(k1 < k3, "Key ordering not transitive");
        }
    }

    #[test]
    fn put_get_roundtrip(
        key in arbitrary_key(),
        value in arbitrary_value()
    ) {
        let storage = UnifiedStore::new();
        storage.put(key.clone(), value.clone(), None).unwrap();
        let retrieved = storage.get(&key).unwrap().unwrap();
        assert_eq!(retrieved.value, value);
    }
}
```

#### 5.3 Run Property Tests
```bash
# Run with many iterations to find edge cases
PROPTEST_CASES=10000 cargo test --all proptest

# Run with different random seeds
for seed in {1..100}; do
  PROPTEST_RNG_SEED=$seed cargo test --all proptest
done
```

### Deliverable
**Property Testing Report**:
- List of properties tested
- Any violations found
- Edge cases discovered

---

## Phase 6: Integration Testing Scenarios

### Objective
Test realistic end-to-end scenarios that cross module boundaries.

### Methodology

#### 6.1 Define Critical Scenarios

**Scenario 1: Concurrent Access**
- Multiple threads writing to same run
- Verify no data races or corruption
- Check version numbers sequential

**Scenario 2: Large Dataset**
- 1M key-value pairs
- Verify performance acceptable
- Check memory usage reasonable

**Scenario 3: Crash Recovery**
- Write 10K entries
- Simulate crash (kill process)
- Reopen database
- Verify all committed data recovered

**Scenario 4: TTL Under Load**
- Write 100K entries with short TTL
- Verify TTL cleanup doesn't interfere with operations
- Check no memory leaks

**Scenario 5: Secondary Index Consistency**
- Run multiple operations
- Verify run_index and type_index always consistent with main store
- Delete entries and verify indices updated

#### 6.2 Implement Integration Tests
Create: `crates/integration-tests/tests/scenarios.rs`

```rust
#[test]
fn scenario_concurrent_writes() {
    // 10 threads, 1000 writes each
    // Verify all 10K entries present
    // Verify no corruption
}

#[test]
fn scenario_large_dataset() {
    // Write 1M entries
    // Measure time (should be <60s)
    // Measure memory (should be <500MB)
}

#[test]
#[ignore] // Run manually
fn scenario_crash_recovery() {
    // Requires actual process fork/kill
}
```

### Deliverable
**Integration Testing Report**:
- All scenarios pass/fail status
- Performance metrics
- Any unexpected behavior

---

## Phase 7: Manual Code Review (Human Review)

### Objective
Human review of critical code sections that tools can't catch.

### Focus Areas

#### 7.1 Core Types Review
**File**: `crates/core/src/types.rs`
- [ ] RunId generation truly unique (UUID v4)
- [ ] Namespace fields validated (no empty strings)
- [ ] Key ordering matches BTreeMap semantics
- [ ] TypeTag exhaustive (all variants handled)

**File**: `crates/core/src/value.rs`
- [ ] Value enum serialization correct for all variants
- [ ] VersionedValue version always incremented
- [ ] Timestamp generation monotonic

#### 7.2 Storage Layer Review
**File**: `crates/storage/src/unified.rs`
- [ ] RwLock usage correct (no deadlocks possible)
- [ ] Version counter atomic (no races)
- [ ] Secondary indices always updated atomically with main store
- [ ] TTL expiration doesn't race with gets

**File**: `crates/storage/src/ttl.rs` (if exists)
- [ ] TTL cleanup uses transactions (not direct mutations)
- [ ] Expired entries don't cause crashes
- [ ] TTL thread shutdown clean

#### 7.3 Error Handling Review
- [ ] No `unwrap()` or `expect()` in production code
- [ ] All `Result` types propagated correctly
- [ ] Error messages informative (include context)
- [ ] No silent failures (errors logged or returned)

#### 7.4 Unsafe Code Review (if any)
```bash
# Find all unsafe blocks
rg "unsafe" crates/ --type rust

# Each unsafe block MUST have:
# - Comment explaining why unsafe needed
# - Comment explaining safety invariants
# - Proof that invariants upheld
```

### Deliverable
**Manual Review Report**:
- Issues found per category
- Risk assessment (HIGH/MEDIUM/LOW)
- Recommended fixes

---

## Phase 8: Bug Reproduction Test Suite

### Objective
Create tests for any bugs found during audit, ensuring they're fixed and stay fixed.

### Methodology

#### 8.1 Create Regression Test File
`crates/core/tests/regression_tests.rs`
`crates/storage/tests/regression_tests.rs`

#### 8.2 Document Bug Pattern
For each bug found:
```rust
/// Bug #1: [Brief description]
///
/// **Discovered**: [Date]
/// **Symptom**: [What went wrong]
/// **Root Cause**: [Why it happened]
/// **Fix**: [How it was fixed]
/// **Test**: Ensures bug doesn't reoccur
#[test]
fn regression_bug_001_description() {
    // Minimal reproduction case
    // Assert fix works
}
```

### Deliverable
**Regression Test Suite**:
- One test per bug found
- All tests passing
- Documentation of each bug

---

## Phase 9: Documentation Audit

### Objective
Ensure code behavior matches documentation.

### Checklist
- [ ] README.md accuracy
- [ ] Rustdoc comments correct (no outdated docs)
- [ ] Architecture docs match implementation
- [ ] TDD_METHODOLOGY.md followed
- [ ] No conflicting information

### Methodology
```bash
# Check rustdoc builds without warnings
cargo doc --all --no-deps 2>&1 | tee rustdoc-warnings.log

# Check for TODO/FIXME in docs
rg "TODO|FIXME" crates/ --type rust -g '!tests'

# Verify doc examples compile
cargo test --doc --all
```

### Deliverable
**Documentation Audit Report**:
- List of documentation issues
- Outdated sections
- Missing documentation

---

## Quality Gates (Must Pass Before Epic 5)

### Gate 1: Test Integrity ‚úÖ or ‚ùå
- [ ] No tests adjusted to hide bugs (Phase 1)
- [ ] All known bugs documented with regression tests (Phase 8)

**If FAIL**: Stop immediately, fix bugs, restore proper tests.

### Gate 2: Test Coverage ‚úÖ or ‚ùå
- [ ] Core types: ‚â•100% line coverage (Phase 3)
- [ ] Storage layer: ‚â•95% line coverage (Phase 3)
- [ ] All error paths tested (Phase 3)

**If FAIL**: Write missing tests until targets met.

### Gate 3: Test Quality ‚úÖ or ‚ùå
- [ ] Mutation score ‚â•80% for core and storage (Phase 4)
- [ ] No flaky tests (Phase 2)
- [ ] All integration scenarios pass (Phase 6)

**If FAIL**: Strengthen tests, remove flaky tests.

### Gate 4: Code Quality ‚úÖ or ‚ùå
- [ ] No `unwrap()`/`expect()` in production code (Phase 7)
- [ ] All `unsafe` justified and documented (Phase 7)
- [ ] No silent failures (Phase 7)

**If FAIL**: Refactor code to meet quality standards.

### Gate 5: Documentation ‚úÖ or ‚ùå
- [ ] All public APIs documented (Phase 9)
- [ ] No outdated documentation (Phase 9)
- [ ] Doc tests compile (Phase 9)

**If FAIL**: Update documentation.

---

## Execution Plan

### Timeline
**Estimated**: 3-4 days (with 1-2 people)

| Phase | Time | Owner | Blocker |
|-------|------|-------|---------|
| 1. Test Integrity Audit | 4 hours | You/Claude | None |
| 2. Full Test Validation | 2 hours | You/Claude | None |
| 3. Coverage Analysis | 3 hours | You/Claude | None |
| 4. Mutation Testing | 6 hours | You/Claude | Phase 2 |
| 5. Property Testing | 4 hours | You/Claude | Phase 3 |
| 6. Integration Tests | 6 hours | You/Claude | Phase 2 |
| 7. Manual Review | 8 hours | You | None |
| 8. Bug Fixes + Regression Tests | Variable | You/Claude | Phases 1-7 |
| 9. Documentation Audit | 2 hours | You/Claude | None |

**Total**: ~35 hours (can parallelize some phases)

### Parallelization
- Phase 1, 2, 3, 9 can run in parallel (different tools)
- Phase 4, 5, 6 depend on Phase 2 (need passing tests)
- Phase 8 runs as bugs are found (ongoing)

### Resources Needed
- `cargo-tarpaulin` (coverage)
- `cargo-mutants` or `cargo-mutagen` (mutation testing)
- `proptest` (property-based testing)
- Human reviewer for Phase 7

---

## Success Criteria

**M1 Foundation is TRUSTED when**:
1. ‚úÖ All 5 quality gates passed
2. ‚úÖ Test coverage ‚â•95% for core components
3. ‚úÖ Mutation score ‚â•80%
4. ‚úÖ No known bugs without regression tests
5. ‚úÖ All integration scenarios pass
6. ‚úÖ Documentation matches implementation
7. ‚úÖ Clean git history (no test workarounds)

**Only then** proceed to Epic 5.

---

## Risk Assessment

### If We Skip This Audit

**Likelihood**: HIGH that bugs exist in Epics 1-2

**Impact**:
- üî¥ **CRITICAL** - Foundation bugs will cascade
- üî¥ **CRITICAL** - Technical debt compounds exponentially
- üî¥ **CRITICAL** - Later bugs much harder to fix
- üî¥ **CRITICAL** - May require rewriting M1 later

**Cost of Skipping**: 10x more expensive to fix later

### If We Execute This Audit

**Likelihood**: MEDIUM that we find bugs

**Impact**:
- üü¢ **POSITIVE** - Bugs caught early
- üü¢ **POSITIVE** - Confidence in foundation
- üü¢ **POSITIVE** - Clean base for M2-M5
- üü¢ **POSITIVE** - Better testing culture established

**Cost of Executing**: 3-4 days now vs weeks later

---

## Recommendation

**PAUSE Epic 5 implementation.**

**EXECUTE this quality audit immediately.**

**Rationale**:
- M1 is the foundation - it MUST be solid
- Epics 3-4 had good testing discipline (95%+ coverage, mutation testing)
- Epics 1-2 predate TDD rigor - need verification
- 3-4 day investment now saves weeks later
- Sets quality bar for all future work

---

## Appendix: Tools and Commands

### Coverage Tools
```bash
# Install
cargo install cargo-tarpaulin cargo-llvm-cov

# HTML report
cargo tarpaulin --all --out Html --output-dir coverage

# JSON for analysis
cargo tarpaulin --all --out Json --output-dir coverage
```

### Mutation Testing
```bash
# Install
cargo install cargo-mutants

# Run
cargo mutants --package in-mem-core
cargo mutants --package in-mem-storage
```

### Property Testing
```bash
# Add to Cargo.toml
[dev-dependencies]
proptest = "1.0"

# Run with many cases
PROPTEST_CASES=10000 cargo test
```

### Benchmark Tools
```bash
# Install
cargo install cargo-criterion

# Run benchmarks
cargo criterion
```

---

**Document Version**: 1.0
**Last Updated**: 2026-01-11
**Next Review**: After Phase 8 completion
