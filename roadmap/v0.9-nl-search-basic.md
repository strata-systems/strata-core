# v0.9: Natural Language Search (Basic)

**Theme**: Ask a question, get results. Qwen3 translates natural language into typed sub-queries.

This is the first LLM integration. Qwen3-1.3B runs inside `strata-intelligence` and decomposes natural language queries into multi-primitive search operations. Results come back raw — no re-ranking, no synthesis. The value is in query understanding, not answer generation.

---

## Query Decomposition

User asks a natural language question. Qwen3 translates it into typed sub-queries across primitives. The existing hybrid search engine (v0.8) executes them.

```
"what tools did the agent use in the last hour?"
        │
        ▼
   Qwen3: query decomposition
   → event search: type=tool_call, time > 1h ago
   → KV search: prefix "agent:tool:"
   → vector search: semantic similarity to "tools agent used"
        │
        ▼
   Enhanced hybrid search (v0.8)
        │
        ▼
   Raw results (no re-ranking, no synthesis)
```

### What Qwen3 does in this release

- Parses natural language into typed sub-queries across primitives
- Maps temporal expressions ("last hour", "yesterday", "before the error") to time ranges
- Identifies which primitives to search based on the query intent
- Generates appropriate filter conditions (metadata filters, key prefixes, event types)

### What Qwen3 does NOT do yet

- No result re-ranking (results scored by existing BM25/vector/RRF pipeline)
- No answer synthesis (no generated text in the response)
- No query expansion (no synonym generation or concept broadening)
- No multi-step retrieval (no "search, then search based on those results")

---

## Qwen3 Runtime: Edge Inference

Qwen3 runs on StrataHub's edge infrastructure (see [edge architecture](./strata-cloud-sync-edge-architecture.md)), not inside the embedded database. Query decomposition is a stateless function — it takes a natural language string and returns typed sub-queries. It doesn't need access to the database.

```
Client (embedded)                       StrataHub Edge
    │                                        │
    ├── NL query ───────────────────────►    │
    │                                        ├── Qwen3 inference
    │   ◄── sub-queries ───────────────────┤   (Workers AI / GPU)
    │                                        │
    ├── Execute locally (hybrid search)      │
    └── Return results                       │
```

- **Model**: Qwen3-1.3B (Q4 quantized), hosted at the edge
- **Client footprint**: Zero additional RAM for NL search. MiniLM (~80 MB) stays on-device for auto-embed.
- **Latency**: ~50-100ms round-trip for query decomposition
- **Offline degradation**: If the edge is unreachable, NL search is unavailable. Keyword + vector search still work locally — you lose query understanding, not the search layer.

### On-device fallback

For air-gapped or privacy-sensitive deployments, an optional feature flag loads Qwen3 locally:

```toml
[features]
intelligence-llm-local = []    # On-device Qwen3 (~1.4 GB additional RAM)
```

When enabled, the client loads Qwen3 locally and skips the edge call. This is the same model, same decomposition logic, just running in-process instead of at the edge. Runtime options for local mode:

| Option | Pros | Cons |
|--------|------|------|
| **candle** (Rust-native) | Pure Rust, no C++ deps | Fewer optimized kernels for LLM generation |
| **llama.cpp** (FFI) | Battle-tested quantization, broad hardware support | C++ build dependency |

The default path is edge inference. Local inference is opt-in for specific deployment constraints.

### Auth

NL inference uses the same JWT auth as sync. The `infer` action scope authorizes query decomposition:

```json
{
    "sub": "agent-a",
    "scope": "acme/agent-memory",
    "actions": ["pull", "infer"],
    "exp": 1720000000
}
```

Agents without the `infer` scope can still search — they just can't use natural language queries.

---

## Memory footprint

### Default (edge inference)

| Component | RAM |
|-----------|-----|
| MiniLM-L6-v2 (from v0.7) | ~80 MB |
| Qwen3 | 0 MB (runs at edge) |
| **Total** | **~80 MB** |

### Local fallback (intelligence-llm-local)

| Component | RAM |
|-----------|-----|
| MiniLM-L6-v2 (from v0.7) | ~80 MB |
| Qwen3-1.3B (Q4) | ~1 GB |
| KV cache (short context) | ~100-200 MB |
| **Total** | **~1.4 GB** |

## Dependencies

- v0.8 (enhanced hybrid search — Qwen3 generates queries that the hybrid pipeline executes)
- StrataHub edge infrastructure (for default edge inference path)

## Open questions

- How much context to feed Qwen3 — just the query string, or recent search/conversation history?
- What prompt template works best for query decomposition?
- How to handle queries that don't map cleanly to primitive operations?
- Should the edge cache recent decompositions for the same project to reduce inference cost?
